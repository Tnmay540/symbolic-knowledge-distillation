{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14714233,"sourceType":"datasetVersion","datasetId":9401000},{"sourceId":14719856,"sourceType":"datasetVersion","datasetId":9405188},{"sourceId":740118,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":564587,"modelId":577079}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==========================================\n# INSTALL DEPENDENCIES\n# ==========================================\n!pip install transformers datasets accelerate\n\nimport json\nimport torch\nfrom transformers import (\n    GPT2Tokenizer, \n    GPT2LMHeadModel, \n    Trainer, \n    TrainingArguments, \n    DataCollatorForLanguageModeling\n)\nfrom datasets import Dataset\n\n# ==========================================\n# CONFIGURATION\n# ==========================================\n\n# 1. Path to your dataset (The output from Step 2/3)\nDATASET_PATH = \"/kaggle/input/tanmay-soni2/unique_dataset (1)_filtered_threshold_0.5.jsonl\" \n\n# 2. Student Model (We use GPT-2 Small as the student, as per the paper)\nMODEL_CHECKPOINT = \"gpt2\" \n\n# 3. Output directory for the trained model\nOUTPUT_DIR = \"/kaggle/working/my_comet_student\"\n\n# ==========================================\n# DATA PREPARATION\n# ==========================================\n\ndef format_comet_input(head, relation, tail):\n    \"\"\"\n    Formats data into the COMET input string:\n    <head> Event </head> <relation> Relation </relation> [GEN] Tail\n    \"\"\"\n    return f\"<head> {head} </head> <relation> {relation} </relation> [GEN] {tail}\"\n\nprint(\"Loading and formatting dataset...\")\ndata = []\nwith open(DATASET_PATH, 'r') as f:\n    for line in f:\n        try:\n            d = json.loads(line)\n            # Use 'inference' (from your unique_dataset) or 'tail'\n            tail_text = d.get('inference', d.get('tail', ''))\n            \n            formatted_text = format_comet_input(d['head'], d['relation'], tail_text)\n            data.append({\"text\": formatted_text})\n        except:\n            continue\n\n# Convert to Hugging Face Dataset\ndataset = Dataset.from_list(data)\n# Split into Train/Val (90% train, 10% val)\ndataset = dataset.train_test_split(test_size=0.1)\n\nprint(f\"Training on {len(dataset['train'])} examples, Validating on {len(dataset['test'])}\")\n\n# ==========================================\n# TOKENIZATION\n# ==========================================\n\nprint(\"Loading Tokenizer...\")\ntokenizer = GPT2Tokenizer.from_pretrained(MODEL_CHECKPOINT)\n\n# GPT-2 doesn't have a pad token, so we add one\ntokenizer.pad_token = tokenizer.eos_token\n\n# Add Special Tokens (Crucial for COMET)\nspecial_tokens_dict = {\n    'additional_special_tokens': ['<head>', '</head>', '<relation>', '</relation>', '[GEN]']\n}\nnum_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=64)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\n# ==========================================\n# TRAINING\n# ==========================================\n\nprint(\"Loading Model...\")\nmodel = GPT2LMHeadModel.from_pretrained(MODEL_CHECKPOINT)\n\n# Resize embeddings because we added special tokens\nmodel.resize_token_embeddings(len(tokenizer))\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    # REPLACE 'evaluation_strategy' WITH 'eval_strategy'\n    eval_strategy=\"epoch\",  \n    learning_rate=5e-5,\n    weight_decay=0.01,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    save_strategy=\"epoch\",\n    save_total_limit=2,\n    fp16=True,\n    report_to=\"none\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\n\nprint(\"Starting Training...\")\ntrainer.train()\n\n# ==========================================\n# SAVING\n# ==========================================\n\nprint(f\"Saving model to {OUTPUT_DIR}...\")\ntrainer.save_model(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\nprint(\"Done! You can now use this model for inference.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nimport os\n\n# 1. Define the folder you want to download (from your config)\nfolder_path = \"/kaggle/working/my_comet_student\"\n\n# 2. Define the output zip filename\nzip_name = \"/kaggle/working/student_model2\"\n\n# 3. Create the zip file\nprint(f\"Zipping {folder_path}...\")\nshutil.make_archive(zip_name, 'zip', folder_path)\n\nprint(f\"Success! Created {zip_name}.zip\")\nprint(\"Check the 'Output' section on the right sidebar to download it.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\nimport os\n\n# 1. Define the file path (based on your previous step)\nzip_file_path = \"student_model2.zip\"\n\n# 2. Check if it exists\nif os.path.exists(zip_file_path):\n    print(f\"File found: {zip_file_path}\")\n    print(\"Click the link below to download:\")\n    \n    # 3. Generate the link\n    display(FileLink(zip_file_path))\nelse:\n    print(f\"Error: Could not find {zip_file_path}\")\n    print(\"Did you run the 'shutil.make_archive' cell above?\")\n    print(\"Current files in directory:\", os.listdir(\".\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T11:05:04.829505Z","iopub.execute_input":"2026-02-04T11:05:04.830251Z","iopub.status.idle":"2026-02-04T11:05:04.838318Z","shell.execute_reply.started":"2026-02-04T11:05:04.830221Z","shell.execute_reply":"2026-02-04T11:05:04.837690Z"}},"outputs":[{"name":"stdout","text":"Error: Could not find student_model2.zip\nDid you run the 'shutil.make_archive' cell above?\nCurrent files in directory: ['.virtual_documents']\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ==========================================\n# 1. SETUP & INSTALLATION\n# ==========================================\n# Run this cell to install dependencies if not already present\n!pip install -q transformers torch\n\nimport torch\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\n\n# ==========================================\n# 2. CONFIGURATION\n# ==========================================\n\n# Path to your trained model folder\n# (Use the one you confirmed works in your previous turns)\nMODEL_PATH = \"/kaggle/input/student-model2/pytorch/default/1\"\n\n# Detect GPU\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n# ==========================================\n# 3. LOAD MODEL & RECONSTRUCT TOKENIZER\n# ==========================================\n\nprint(f\"Loading model weights from {MODEL_PATH}...\")\ntry:\n    # Load the Model Weights\n    model = GPT2LMHeadModel.from_pretrained(MODEL_PATH).to(device)\n    model.eval()\n\n    print(\"Reconstructing Tokenizer from base 'gpt2'...\")\n    # Load base GPT-2 tokenizer (since your folder lacked vocab.json)\n    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n    tokenizer.pad_token = tokenizer.eos_token\n\n    # CRITICAL: Re-add special tokens in the EXACT order used during training\n    # This aligns the tokenizer's vocabulary with your trained model\n    special_tokens_dict = {\n        'additional_special_tokens': ['<head>', '</head>', '<relation>', '</relation>', '[GEN]']\n    }\n    tokenizer.add_special_tokens(special_tokens_dict)\n    \n    print(\"‚úÖ Model and Tokenizer loaded successfully!\")\n\nexcept Exception as e:\n    print(f\"‚ùå Error loading model: {e}\")\n    # Stop execution if model fails to load\n    raise e\n\n# ==========================================\n# 4. INTERACTIVE CHATBOT FUNCTION\n# ==========================================\ndef run_full_report_chat():\n    print(\"\\n\" + \"=\"*60)\n    print(\"ü§ñ COMET REPORT GENERATOR\")\n    print(\"=\"*60)\n    print(\"Type an Event, and I will generate ALL social inferences for it.\")\n    \n    relations_to_check = [\n        ('xIntent', 'Why it happened'),\n        ('xEffect', 'What happens next'),\n        ('xReact',  'How they feel'),\n        ('xAttr',   'Personality trait'),\n        ('xWant',   'What they want next'),\n        ('xNeed',   'What PersonX needed to do before'),\n        ('HinderedBy', 'What could prevent this')\n    ]\n\n    while True:\n        head = input(\"\\nüìù Enter Event (or 'exit'): \").strip()\n        if head.lower() in ['exit', 'quit']: break\n        \n        print(f\"\\n--- Analysis for: '{head}' ---\")\n        \n        for rel_code, rel_desc in relations_to_check:\n            # Prepare input\n            input_text = f\"<head> {head} </head> <relation> {rel_code} </relation> [GEN]\"\n            inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n            \n            # Generate\n            with torch.no_grad():\n                outputs = model.generate(\n                    **inputs, \n                   max_new_tokens=20,        # FORCE it to be short (Atomic is rarely >10 words)\n    min_length=2,             # Prevent empty answers\n    num_beams=5,              # Higher beams = smarter search\n    no_repeat_ngram_size=2,   # KILL repetition (e.g. \"immature and immature\")\n    early_stopping=True,      # Stop exactly when the EOS token is found\n    pad_token_id=tokenizer.eos_token_id,\n    eos_token_id=tokenizer.eos_token_id, # Explicitly tell it what \"Stop\" looks like\n    repetition_penalty=1.5    # Strong penalty for repeating phrases\n                )\n            \n            # Decode\n            full_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n            inference = full_text.split('[GEN]')[1].split('<')[0].strip()\n            \n            # Print cleanly\n            print(f\"üîπ {rel_desc:<20} ({rel_code}): {inference}\")\n\ndef run_comet_chat():\n    print(\"\\n\" + \"=\"*60)\n    print(\"ü§ñ COMET STUDENT MODEL - INTERACTIVE DEMO\")\n    print(\"=\"*60)\n    print(\"Instructions:\")\n    print(\"1. Type an Event (e.g., 'PersonX asks for a refund')\")\n    print(\"2. Choose a Relation from the list below.\")\n    print(\"3. Type 'exit' or 'quit' at any time to stop.\")\n    print(\"-\" * 60)\n    \n    valid_relations = [\n        'xWant',      # What PersonX wants to do after\n        'xEffect',    # Effect on PersonX\n        'xReact',     # How PersonX feels\n        'xAttr',      # How PersonX is seen (attributes)\n        'xNeed',      # What PersonX needed to do before\n        'xIntent',    # Why PersonX did this\n        'HinderedBy'  # What could prevent this\n    ]\n\n    while True:\n        # --- INPUT EVENT ---\n        print(\"\\n\" + \"-\"*30)\n        head = input(\"üìù Enter Event: \").strip()\n        \n        if head.lower() in ['exit', 'quit']:\n            print(\"Goodbye! üëã\")\n            break\n        if not head:\n            continue\n\n        # --- INPUT RELATION ---\n        print(f\"   Available Relations: {', '.join(valid_relations)}\")\n        relation = input(\"üîó Enter Relation: \").strip()\n        \n        if relation.lower() in ['exit', 'quit']:\n            print(\"Goodbye! üëã\")\n            break\n\n        # --- GENERATION ---\n        try:\n            # Format the input exactly as the model was trained\n            input_text = f\"<head> {head} </head> <relation> {relation} </relation> [GEN]\"\n            inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n            \n            # Generate Inference\n            with torch.no_grad():\n                outputs = model.generate(\n                    **inputs,\n                    max_new_tokens=24,       # Short, crisp inferences\n                    num_beams=5,             # Beam search for quality\n                    early_stopping=True,\n                    no_repeat_ngram_size=2,\n                    pad_token_id=tokenizer.eos_token_id\n                )\n            \n            # Decode output\n            full_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n            \n            # Parse the result (Extract text after [GEN])\n            try:\n                inference = full_text.split('[GEN]')[1].strip()\n                # Clean up specific end tokens if they appear\n                inference = inference.replace('<|endoftext|>', '').strip()\n            except IndexError:\n                inference = full_text # Fallback if structure is malformed\n\n            # Output\n            print(\"=\"*60)\n            print(f\"üß† Inference:  {inference}\")\n            print(\"=\"*60)\n\n        except Exception as e:\n            print(f\"‚ùå Error during generation: {e}\")\n\n# ==========================================\n# 5. START THE CHAT\n# ==========================================\nif __name__ == \"__main__\":\n    run_full_report_chat()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T11:20:48.450437Z","iopub.execute_input":"2026-02-04T11:20:48.450768Z","iopub.status.idle":"2026-02-04T11:21:58.387362Z","shell.execute_reply.started":"2026-02-04T11:20:48.450739Z","shell.execute_reply":"2026-02-04T11:21:58.386404Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading model weights from /kaggle/input/student-model2/pytorch/default/1...\nReconstructing Tokenizer from base 'gpt2'...\n‚úÖ Model and Tokenizer loaded successfully!\n\n============================================================\nü§ñ COMET REPORT GENERATOR\n============================================================\nType an Event, and I will generate ALL social inferences for it.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nüìù Enter Event (or 'exit'):  john tries to get a new car\n"},{"name":"stdout","text":"\n--- Analysis for: 'john tries to get a new car' ---\nüîπ Why it happened      (xIntent): to be able to afford a car for himself and his family. PersonX wants to drive the car\nüîπ What happens next    (xEffect): learns to drive a car and is able to afford it, even though he has no money to buy\nüîπ How they feel        (xReact): excited about the car and wants to drive it for a while longer than he has to go to school\nüîπ Personality trait    (xAttr): confident in his car skills and wants to buy a car for himself. PersonX is not interested in\nüîπ What they want next  (xWant): to buy a car for himself and drive it for a friend who is willing to help him out with\nüîπ What PersonX needed to do before (xNeed): to buy a car and drive it well enough to be able to afford it. PersonX can't\nüîπ What could prevent this (HinderedBy): PersonX can't find a car dealer's listing for the car he wants to buy for himself.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/4280972593.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# ==========================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0mrun_full_report_chat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_55/4280972593.py\u001b[0m in \u001b[0;36mrun_full_report_chat\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mhead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nüìù Enter Event (or 'exit'): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'exit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"],"ename":"KeyboardInterrupt","evalue":"Interrupted by user","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"# ==========================================\n# 1. INSTALL METRICS\n# ==========================================\n!pip install evaluate rouge_score nltk\n\nimport json\nimport torch\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nimport evaluate\nfrom tqdm import tqdm\n\n# ==========================================\n# 2. CONFIGURATION\n# ==========================================\n\n# Path to your TRAINED student model (output of the previous step)\nMODEL_PATH = \"/kaggle/input/student-model2/pytorch/default/1\"\n\n# Path to your data\nDATA_PATH = \"/kaggle/input/tanmay-soni2/unique_dataset (1)_filtered_threshold_0.5.jsonl\"\n\n# Use GPU if available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# ==========================================\n# 3. LOAD MODEL & RECONSTRUCT TOKENIZER\n# ==========================================\n\nprint(f\"Loading model weights from {MODEL_PATH}...\")\n# 1. Load the Model Weights (This works because the model file is there)\nmodel = GPT2LMHeadModel.from_pretrained(MODEL_PATH).to(device)\nmodel.eval()\n\nprint(\"Reconstructing Tokenizer from base 'gpt2'...\")\n# 2. Load the base GPT-2 tokenizer (downloading fresh from Hugging Face)\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token\n\n# 3. RE-ADD SPECIAL TOKENS\n# CRITICAL: These must be added in the EXACT same order as your training script\n# so that they get the same IDs (indices) that the model learned.\nspecial_tokens_dict = {\n    'additional_special_tokens': ['<head>', '</head>', '<relation>', '</relation>', '[GEN]']\n}\ntokenizer.add_special_tokens(special_tokens_dict)\n\nprint(\"Tokenizer reconstructed and synchronized.\")\n\nprint(\"Loading Test Data...\")\n# We need to group references: (Head + Relation) -> [List of valid inferences]\ntest_data = {}\n\nwith open(DATA_PATH, 'r') as f:\n    for line in f:\n        try:\n            d = json.loads(line)\n            if d.get('split') == 'test':\n                key = (d['head'], d['relation'])\n                inference = d.get('inference', d.get('tail', ''))\n                \n                if key not in test_data:\n                    test_data[key] = []\n                test_data[key].append(inference)\n        except:\n            continue\n\nprint(f\"Found {len(test_data)} unique test prompts.\")\n# ==========================================\n# 4. GENERATION FUNCTION\n# ==========================================\n\ndef generate_inference(head, relation):\n    # Format input exactly like training: <head> ... <relation> ... [GEN]\n    input_text = f\"<head> {head} </head> <relation> {relation} </relation> [GEN]\"\n    \n    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=24,      # Inferences are usually short\n            num_beams=3,            # Beam search for better quality\n            early_stopping=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    # Decode and remove the input prompt part\n    full_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n    # Extract text after [GEN]\n    try:\n        generated_text = full_text.split('[GEN]')[1].strip()\n        # Remove special tokens if they leaked\n        generated_text = generated_text.replace('<|endoftext|>', '').strip()\n    except:\n        generated_text = full_text # Fallback\n        \n    return generated_text\n\n# ==========================================\n# 5. RUN EVALUATION LOOP\n# ==========================================\n\npredictions = []\nreferences = []\n\nprint(\"Generating predictions (This may take a few minutes)...\")\n# Limit to first 200 for speed if you want a quick check, or remove [:200] for full eval\ntest_keys = list(test_data.keys()) # [:200] \n\nfor head, relation in tqdm(test_keys):\n    # 1. Generate\n    pred = generate_inference(head, relation)\n    predictions.append(pred)\n    \n    # 2. Get References (Teacher's answers)\n    refs = test_data[(head, relation)]\n    references.append(refs)\n\n# ==========================================\n# 6. CALCULATE SCORES\n# ==========================================\n\nbleu = evaluate.load(\"bleu\")\nrouge = evaluate.load(\"rouge\")\n\nprint(\"\\nCalculating Metrics...\")\n\n# BLEU\nbleu_score = bleu.compute(predictions=predictions, references=references)\nprint(f\"BLEU Score: {bleu_score['bleu']:.4f}\")\n\n# ROUGE\nrouge_score = rouge.compute(predictions=predictions, references=references)\nprint(f\"ROUGE-L: {rouge_score['rougeL']:.4f}\")\n\n# ==========================================\n# 7. QUALITATIVE CHECK (SEE EXAMPLES)\n# ==========================================\n\nprint(\"\\n=== QUALITATIVE EXAMPLES ===\")\nfor i in range(5):\n    head, rel = test_keys[i]\n    print(f\"Event:    {head}\")\n    print(f\"Relation: {rel}\")\n    print(f\"Student:  {predictions[i]}\")\n    print(f\"Teacher:  {test_data[(head, rel)]}\")\n    print(\"-\" * 40)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers==4.30.0 torch accelerate bitsandbytes\n!pip install ftfy names scipy scikit-learn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install -U bitsandbytes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport torch\nimport evaluate\nfrom tqdm import tqdm\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\n# ==========================================\n# 1. LOAD TEACHER (4-Bit Quantized)\n# ==========================================\nteacher_name = \"EleutherAI/gpt-j-6B\"\n\nprint(f\"Loading Teacher: {teacher_name}...\")\n\ntokenizer = AutoTokenizer.from_pretrained(teacher_name)\ntokenizer.pad_token = tokenizer.eos_token # GPT-J needs a pad token\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    teacher_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n)\n\n# ==========================================\n# 2. DEFINE PROMPT TEMPLATES\n# ==========================================\n# The Teacher is a base model, so we prompt it with sentence completion.\n# These map the Relation to a natural language phrase.\nRELATION_TEMPLATES = {\n    'xWant': \"{head}, as a result, PersonX wants\",\n    'xEffect': \"{head}, as a result, PersonX\",\n    'xReact': \"{head}, as a result, PersonX feels\",\n    'xIntent': \"{head}, because PersonX wanted\",\n    'xNeed': \"{head}, but before, PersonX needed\",\n    'xAttr': \"{head}, so PersonX is seen as\",\n    'HinderedBy': \"{head}. This can be hindered by\",\n}\n\ndef construct_teacher_prompt(head, relation):\n    # Get the template or default to a generic one\n    template = RELATION_TEMPLATES.get(relation, \"{head} {relation}\")\n    # Fill in the event\n    prompt = template.format(head=head, relation=relation)\n    return prompt\n\n# ==========================================\n# 3. LOAD TEST DATA\n# ==========================================\nDATA_PATH = \"/kaggle/input/tanmay-soni2/unique_dataset (1)_filtered_threshold_0.5.jsonl\" # Your dataset\n\nprint(\"Loading Test Data...\")\ntest_data = {}\n\n# We group references by prompt so we can compare 1 Gen vs All Valid Refs\nwith open(DATA_PATH, 'r') as f:\n    for line in f:\n        try:\n            d = json.loads(line)\n            # Only look at the TEST split\n            if d.get('split') == 'test':\n                # Key = (Head, Relation)\n                key = (d['head'], d['relation'])\n                inference = d.get('inference', d.get('tail', ''))\n                \n                if key not in test_data:\n                    test_data[key] = []\n                test_data[key].append(inference)\n        except:\n            continue\n\nprint(f\"Found {len(test_data)} unique test prompts.\")\n\n# ==========================================\n# 4. EVALUATION LOOP\n# ==========================================\npredictions = []\nreferences = []\n\nprint(\"Generating Teacher predictions...\")\n# We limit to 50 for speed demonstration (Remove [:50] for full run)\ntest_keys = list(test_data.keys())[:50] \n\nfor head, relation in tqdm(test_keys):\n    # A. Format Input (Natural Language)\n    prompt = construct_teacher_prompt(head, relation)\n    \n    # B. Tokenize\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    \n    # C. Generate\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=15,      # Atomic inferences are short\n            do_sample=False,        # Greedy for deterministic eval\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    # D. Decode and Clean\n    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Extract only the newly generated part (remove the prompt)\n    # GPT-J output: \"Prompt + Generation\"\n    generated_text = full_text[len(prompt):].strip()\n    \n    # Simple cleanup (stop at newline or period if needed)\n    generated_text = generated_text.split('\\n')[0].strip()\n    \n    predictions.append(generated_text)\n    references.append(test_data[(head, relation)])\n\n# ==========================================\n# 5. CALCULATE METRICS\n# ==========================================\nbleu = evaluate.load(\"bleu\")\nrouge = evaluate.load(\"rouge\")\n\nprint(\"\\n=== TEACHER RESULTS ===\")\n\n# BLEU\nbleu_score = bleu.compute(predictions=predictions, references=references)\nprint(f\"BLEU Score: {bleu_score['bleu']:.4f}\")\n\n# ROUGE\nrouge_score = rouge.compute(predictions=predictions, references=references)\nprint(f\"ROUGE-L: {rouge_score['rougeL']:.4f}\")\n\n# ==========================================\n# 6. SHOW EXAMPLES\n# ==========================================\nprint(\"\\n=== EXAMPLES ===\")\nfor i in range(3):\n    head, rel = test_keys[i]\n    print(f\"Prompt:   {construct_teacher_prompt(head, rel)}\")\n    print(f\"Teacher:  {predictions[i]}\")\n    print(f\"Refs:     {references[i][:2]}\") # Show first 2 refs\n    print(\"-\" * 30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T14:22:40.000150Z","iopub.execute_input":"2026-02-03T14:22:40.000544Z","iopub.status.idle":"2026-02-03T14:28:09.630367Z","shell.execute_reply.started":"2026-02-03T14:22:40.000504Z","shell.execute_reply":"2026-02-03T14:28:09.629623Z"}},"outputs":[{"name":"stderr","text":"2026-02-03 14:22:44.521728: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1770128564.543158     309 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1770128564.549687     309 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1770128564.567130     309 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770128564.567150     309 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770128564.567153     309 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770128564.567155     309 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"Loading Teacher: EleutherAI/gpt-j-6B...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/619 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c521e0e2e30b457fac3dee7f04fe8723"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bba5800d5ff4fad9a5383fdb2d9b97c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4d89cb20b1d45919b83163bf0788e38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4166593d29a41da91e1f2640e845557"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2abdccdadced4ca9b7788a1128746279"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9baa9702357e446da4243d66144c5f52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/930 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a532d36ffee14147a73b4e59d3578dc9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/24.2G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"111ae9f1ee554e37b8b68d7fd04df50f"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at EleutherAI/gpt-j-6B were not used when initializing GPTJForCausalLM: ['transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.12.attn.bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.13.attn.bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.14.attn.bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.15.attn.bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.16.attn.bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.17.attn.bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.18.attn.bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.19.attn.bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.20.attn.bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.21.attn.bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.22.attn.bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.23.attn.bias', 'transformer.h.23.attn.masked_bias', 'transformer.h.24.attn.bias', 'transformer.h.24.attn.masked_bias', 'transformer.h.25.attn.bias', 'transformer.h.25.attn.masked_bias', 'transformer.h.26.attn.bias', 'transformer.h.26.attn.masked_bias', 'transformer.h.27.attn.bias', 'transformer.h.27.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.masked_bias']\n- This IS expected if you are initializing GPTJForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing GPTJForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"Loading Test Data...\nFound 769 unique test prompts.\nGenerating Teacher predictions...\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:50<00:00,  1.02s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n=== TEACHER RESULTS ===\nBLEU Score: 0.0631\nROUGE-L: 0.3197\n\n=== EXAMPLES ===\nPrompt:   PersonX says something surprising. This can be hindered by\nTeacher:  the fact that the player is not in the same room as the NPC.\nRefs:     ['PersonX is afraid to tell his parents about the baby', \"PersonX doesn't say anything surprising\"]\n------------------------------\nPrompt:   PersonX says something surprising, but before, PersonX needed\nTeacher:  to be a person.\nRefs:     ['to be thoughtful', 'to be honest']\n------------------------------\nPrompt:   PersonX says something surprising, as a result, PersonX wants\nTeacher:  to know what PersonY thinks.\nRefs:     ['to be a good friend', 'to avoid talking to PersonX']\n------------------------------\n","output_type":"stream"}],"execution_count":1}]}