{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14714837,"sourceType":"datasetVersion","datasetId":9401457}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers==4.30.0 torch accelerate bitsandbytes\n!pip install ftfy names scipy scikit-learn\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install -U bitsandbytes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\n# --- 1. Load Teacher (GPT-J) ---\nteacher_name = \"EleutherAI/gpt-j-6B\"\n\nprint(f\"Loading Teacher: {teacher_name}...\")\n\ntokenizer = AutoTokenizer.from_pretrained(teacher_name)\n\n# CRITICAL FIX FOR BATCHING:\n# GPT-J doesn't have a pad token by default. We must set it.\ntokenizer.pad_token = tokenizer.eos_token\n# For generation, we MUST pad on the LEFT so the model generates continuations correctly\ntokenizer.padding_side = \"left\" \n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\nteacher_model = AutoModelForCausalLM.from_pretrained(\n    teacher_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n)\n\nprint(\"Model loaded successfully with Padding Configured!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T04:45:54.781192Z","iopub.execute_input":"2026-02-03T04:45:54.781896Z","iopub.status.idle":"2026-02-03T04:49:56.374268Z","shell.execute_reply.started":"2026-02-03T04:45:54.781860Z","shell.execute_reply":"2026-02-03T04:49:56.373493Z"}},"outputs":[{"name":"stdout","text":"Loading Teacher: EleutherAI/gpt-j-6B...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/24.2G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6263e52af014cb387573030290019c6"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at EleutherAI/gpt-j-6B were not used when initializing GPTJForCausalLM: ['transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.12.attn.bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.13.attn.bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.14.attn.bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.15.attn.bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.16.attn.bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.17.attn.bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.18.attn.bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.19.attn.bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.20.attn.bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.21.attn.bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.22.attn.bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.23.attn.bias', 'transformer.h.23.attn.masked_bias', 'transformer.h.24.attn.bias', 'transformer.h.24.attn.masked_bias', 'transformer.h.25.attn.bias', 'transformer.h.25.attn.masked_bias', 'transformer.h.26.attn.bias', 'transformer.h.26.attn.masked_bias', 'transformer.h.27.attn.bias', 'transformer.h.27.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.masked_bias']\n- This IS expected if you are initializing GPTJForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing GPTJForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"Model loaded successfully with Padding Configured!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\nimport torch.nn.functional as F\n\n# --- Manual Implementation of compute_transition_scores ---\ndef compute_transition_scores(sequences, scores, normalize_logits=True):\n    \"\"\"\n    Fallback implementation for older transformers versions.\n    Calculates the log-probabilities of the selected tokens.\n    \"\"\"\n    # 1. Stack the scores tuple into a single tensor\n    # scores comes as a tuple of tensors (one per step)\n    # Shape becomes: (batch_size, generated_len, vocab_size)\n    scores_stack = torch.stack(scores, dim=1)\n    \n    # 2. Normalize logits to log-probs if requested (NLL requires this)\n    if normalize_logits:\n        scores_stack = F.log_softmax(scores_stack, dim=-1)\n        \n    # 3. Align sequences with scores\n    # sequences contains [prompt + generated]. We only need [generated].\n    gen_len = len(scores)\n    \n    # Take the last 'gen_len' tokens from the sequence\n    generated_ids = sequences[:, -gen_len:]\n    \n    # 4. Gather the scores for the tokens that were actually selected\n    # We use gather to pluck the specific score for each token ID\n    generated_ids = generated_ids.unsqueeze(-1) # Shape: (batch, gen_len, 1)\n    token_scores = torch.gather(scores_stack, 2, generated_ids) # Shape: (batch, gen_len, 1)\n    \n    return token_scores.squeeze(-1) # Shape: (batch, gen_len)\n\ncompute_transition_scores \n\nprint(\"Manual compute_transition_scores defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T04:50:03.846937Z","iopub.execute_input":"2026-02-03T04:50:03.847554Z","iopub.status.idle":"2026-02-03T04:50:03.854809Z","shell.execute_reply.started":"2026-02-03T04:50:03.847523Z","shell.execute_reply":"2026-02-03T04:50:03.853967Z"}},"outputs":[{"name":"stdout","text":"Manual compute_transition_scores defined.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# 1. Define the replacement function with NLL calculation\ndef complete_gpt3(prompt, l, model_name, num_log_probs=None, n=1, top_p=0.9, **kwargs):\n    \"\"\"\n    Replaces OpenAI API call with local teacher_model generation.\n    Handles BATCH inputs (lists of strings) correctly.\n    \"\"\"\n    # CRITICAL: Enable padding for batch processing\n    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(teacher_model.device)\n    input_length = inputs.input_ids.shape[1]\n    \n    with torch.no_grad():\n        outputs = teacher_model.generate(\n            **inputs,\n            max_new_tokens=l,\n            do_sample=True,\n            top_p=top_p,\n            num_return_sequences=n,\n            pad_token_id=tokenizer.eos_token_id,\n            return_dict_in_generate=True,\n            output_scores=True\n        )\n    \n    transition_scores = compute_transition_scores(\n        outputs.sequences, outputs.scores, normalize_logits=True\n    )\n\n    choices = []\n    \n    # Iterate over generated sequences\n    # Note: If batch_size=7 and n=10, we have 70 sequences.\n    # We must be careful to treat them as independent generations for the list.\n    for idx, sequence in enumerate(outputs.sequences):\n        generated_tokens = sequence[input_length:]\n        full_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n        \n        if '\\n' in full_text:\n            generated_text = full_text.split('\\n')[0]\n        else:\n            generated_text = full_text\n\n        current_scores = transition_scores[idx].cpu().numpy().tolist()\n        dummy_offsets = list(range(len(current_scores)))\n        \n        choice_obj = {\n            \"text\": generated_text, \n            \"finish_reason\": \"stop\",\n            \"logprobs\": {\n                \"text_offset\": dummy_offsets, \n                \"token_logprobs\": current_scores \n            }\n        }\n        choices.append(choice_obj)\n\n    return {\"choices\": choices}\n\nprint(\"Advanced Monkey patch applied with BATCH SUPPORT.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T04:50:09.437426Z","iopub.execute_input":"2026-02-03T04:50:09.437960Z","iopub.status.idle":"2026-02-03T04:50:09.445449Z","shell.execute_reply.started":"2026-02-03T04:50:09.437921Z","shell.execute_reply":"2026-02-03T04:50:09.444806Z"}},"outputs":[{"name":"stdout","text":"Advanced Monkey patch applied with BATCH SUPPORT.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"def get_key(source, target):\n    return '{}'.format(json.dumps({'source':source, 'target':target}))\ndef few_shot_prompt(examples, ex2text, number = None, Person_list = None):\n    template_str = ''\n    \n    i = -1\n    for i, example in enumerate(examples[:-1]):\n        \n        if number:\n            ex_str = ex2text(example, include_gen = True, number = i + 1)\n        else:\n            ex_str = ex2text(example, include_gen = True)\n            \n            \n        if Person_list is not None:\n            ex_str = name_PX_PY(ex_str, Person_list[i][0],Person_list[i][1] )\n            \n        template_str += ex_str\n        \n    i = i + 1\n    \n    if number:\n        ex_str = ex2text(examples[-1], include_gen = False,number = i + 1)\n    else:\n        ex_str = ex2text(examples[-1], include_gen = False)\n        \n    if Person_list is not None:\n        ex_str = name_PX_PY(ex_str, Person_list[i][0],Person_list[i][1] )\n    \n    template_str += ex_str\n    \n    return template_str\ndef scrub_PX_PY(s, PersonX, PersonY):\n    return s.replace(PersonX, 'PersonX').replace(PersonY, 'PersonY')\n\ndef name_PX_PY(s, PersonX, PersonY):\n    return s.replace('PersonX', PersonX).replace('PersonY', PersonY)\ndef gpt3(prompt, max_len, model_name, temp=0, num_log_probs=100, echo=False, n=1, **kwargs):\n    \"\"\"\n    Replaces the original gpt3() function to use your local teacher_model.\n    \"\"\"\n    print('calling local model (patch)')\n    \n    # 1. Encode Input\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(teacher_model.device)\n    input_length = inputs.input_ids.shape[1]\n    \n    # 2. Setup Generation Arguments\n    # The original gpt3 function uses temp=0 by default (Greedy Search)\n    do_sample = True\n    if temp == 0:\n        do_sample = False\n        temp = 1.0 # Temperature must be > 0 even if unused, to prevent errors in some configs\n    \n    # 3. Generate\n    with torch.no_grad():\n        outputs = teacher_model.generate(\n            **inputs,\n            max_new_tokens=max_len,\n            do_sample=do_sample,\n            temperature=temp,\n            num_return_sequences=n if n else 1,\n            pad_token_id=tokenizer.eos_token_id,\n            return_dict_in_generate=True,\n            output_scores=True # Crucial for NLL calculation\n        )\n\n    # 4. Calculate Scores (Logprobs)\n    # transition_scores contains the log-probability of each generated token\n    transition_scores = compute_transition_scores(\n        outputs.sequences, outputs.scores, normalize_logits=True\n    )\n\n    # 5. Format Response to mimic OpenAI API\n    choices = []\n    \n    for idx, sequence in enumerate(outputs.sequences):\n        # A. Decode text\n        generated_tokens = sequence[input_length:]\n        full_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n        \n        # B. Handle Stop Token ('\\n') processing manually\n        if '\\n' in full_text:\n            text_output = full_text.split('\\n')[0]\n            # Calculate how many tokens we actually used (approximate for logprobs)\n            # This is a heuristic; exact token alignment after split is complex but \n            # this is sufficient for NLL filtering.\n            stop_index = full_text.find('\\n')\n            # We keep scores for the whole sequence; the notebook will slice them anyway\n        else:\n            text_output = full_text\n\n        # C. Get Logprobs\n        # Convert tensor to list of floats\n        current_scores = transition_scores[idx].cpu().numpy().tolist()\n        \n        # D. Construct Response Object\n        choice_obj = {\n            \"text\": text_output,\n            \"finish_reason\": \"stop\",\n            \"logprobs\": {\n                # Create dummy offsets so the downstream code's `.index(max(...))` logic works\n                \"text_offset\": list(range(len(current_scores))),\n                \"token_logprobs\": current_scores\n            }\n        }\n        choices.append(choice_obj)\n\n    return {\"choices\": choices}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T04:50:15.266002Z","iopub.execute_input":"2026-02-03T04:50:15.266657Z","iopub.status.idle":"2026-02-03T04:50:15.277715Z","shell.execute_reply.started":"2026-02-03T04:50:15.266623Z","shell.execute_reply":"2026-02-03T04:50:15.276990Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"'''\n\nTemplate Definition\n\n'''\n\n\n\n\n'''\n\nADD xWant\n\n'''\n\n'''\n\nADD xWant\n\n'''\n\n##### DEFINE THIS ######\nrelation_to_test = 'xWant'\n########################\n\nexamples_string = \"\"\"PersonX is at a party\txWant\tto drink beer and dance\nPersonX bleeds a lot\txWant\tto see a doctor\nPersonX works as a cashier\txWant\tto be a store manager\nPersonX gets dirty\txWant\tto clean up\nPersonX stays up all night studying\txWant\tto sleep all day\nPersonX gets PersonY's autograph\txWant\tto have a relationship with PersonY\nPersonX ends a friendship\txWant\tto meet new people\nPersonX makes his own costume\txWant\tto go to a costume party\nPersonX calls PersonY\txWant\tto have a long chat\nPersonX tells PersonY a secret\txWant\tto get PersonY's advice\nPersonX mows the lawn\txWant\tto get a new lawnmower\nPersonX makes a huge mistake\txWant\tto apologize\nPersonX sees PersonY's point\txWant\tto agree with PersonY\nPersonX wants a tattoo\txWant\tto find a tattoo design\nPersonX tells PersonY something\txWant\tto get PersonY's opinion\nPersonX leaves PersonY's bike\txWant\tto keep the bike safe\nPersonX visits some friends\txWant\tto catch up with friends\nPersonX sits next to PersonY\txWant\tto become better friends with PersonY\nPersonX gives PersonY indication\txWant\tto date PersonY\nPersonX buys some popcorn\txWant\teat popcorn from bag in car\"\"\"\n    \nlines = [v.split('\\t') for v in  examples_string.split('\\n')]\n\nexamples = [{'head':v[0], 'relation':v[1], 'tail':v[2]} for v in lines]\n\nassert(examples[0]['relation'] == relation_to_test)\n\n\n\ndef ex2text(ex, include_gen = True, number=None):\n\n    text = ''\n    \n    if number is not None:\n        text += 'Situation {}: '.format(number)\n    \n    text += '{}.\\n\\nWants: As a result, PersonX wants'.format(ex['head'],ex['head'])\n\n    if include_gen:\n        text += ' {}.\\n\\n'.format(ex['tail'])\n    return text\n\n\n\nxWant_dict = {'relation':'xWant', \n              'examples':examples[:7],\n             'function':ex2text,\n             'prefix':'How does this situation affect each character\\'s wants?\\n\\n\\n',\n             'length':8}\n\n'''\n\nADD xAttr\n\n'''\n\n##### DEFINE THIS ######\nrelation_to_test = 'xAttr'\n########################\n\nxAttr_examples_string = \"\"\"PersonX bullies PersonY\txAttr\tdominant\nPersonX moves to another city\txAttr\tadventurous\nPersonX changes PersonY's mind\txAttr\tpersuasive\nPersonX writes a story\txAttr\tcreative\nPersonX covers PersonY's expenses\txAttr\tgenerous\nPersonX takes time off\txAttr\tlazy\nPersonX advises PersonY\txAttr\twise\nPersonX bursts into tears\txAttr\tsensitive\nPersonX deals with problems\txAttr\tdetermined\nPersonX follows PersonY\txAttr\tcreepy\"\"\"\n    \nlines = [v.split('\\t') for v in  xAttr_examples_string.split('\\n')]\n\nxAttr_examples = [{'head':v[0], 'relation':'xAttr', 'tail':v[2]} for v in lines]\n\n\n\ndef ex2text(ex, include_gen = True, number=None):\n\n    text = ''\n    \n    if number is not None:\n        text += '{}. '.format(number)\n    \n    text += 'When {}, people think PersonX is'.format(ex['head'])\n\n    if include_gen:\n        text += ' {}.\\n\\n'.format(ex['tail'])\n    return text\n\n\n\n\nxAttr_dict = {'relation':'xAttr', \n              'examples':xAttr_examples[:10],\n             'function':ex2text,\n             'prefix':'Next, we will discuss how people are seen in different situations. Examples:\\n\\n\\n',\n             'length':5}\n\n\n\n'''\n\nADD xIntent\n\n'''\n\n##### DEFINE THIS ######\nrelation_to_test = 'xIntent'\n########################\n\n\n\nexamples_string = \"\"\"PersonX gets the newspaper\txIntent\tto read the newspaper\nPersonX works all night\txIntent\tto meet a deadline\nPersonX destroys PersonY\txIntent\tto punish PersonY\nPersonX clears her mind\txIntent\tto be ready for a new task\nPersonX wants to start a business\txIntent\tto be self sufficient\nPersonX ensures PersonY's safety\txIntent\tto be helpful\nPersonX buys lottery tickets\txIntent\tto become rich\nPersonX chases the ball\txIntent\tto return it to some kids\nPersonX calls customer service\txIntent\tto solve a problem\nPersonX hops around\txIntent\tto exercise\"\"\"\n    \n    \nlines = [v.split('\\t') for v in  examples_string.split('\\n')]\n\nexamples = [{'head':v[0], 'relation':v[1], 'tail':v[2]} for v in lines]\n\n\ndef ex2text(ex, include_gen = True, number=None):\n    text = ''\n    \n    if number is not None:\n        text += '{}. '.format(number)\n    #text += '\\n\\nEvent: {}\\n\\nCharacter: PersonX\\n\\nIntention: \"{}\" because PersonX is trying'.format(ex['head'],ex['head'])\n    text += '\\n\\nEvent: {}.\\n\\nIntention: \"{}\" because PersonX wants'.format(ex['head'],ex['head'])\n\n    if include_gen:\n        text += ' {}.\\n\\n'.format(ex['tail'])\n    return text\n\n\n\nxIntent_dict = {'relation':relation_to_test, \n              'examples':examples[:5],\n             'function':ex2text,\n               'prefix':'For each event, what do the characters want?\\n\\n\\n'}\n\n\n'''\n\nADD xEffect\n\n'''\n\n'''\n\nADD xEffect\n\n'''\n\n##### DEFINE THIS ######\nrelation_to_test = 'xEffect'\n########################\n\n\n\nexamples_string = \"\"\"PersonX recently divorced\txEffect\tis single\nPersonX lifts weights\txEffect\tgains muscle mass\nPersonX takes PersonY to a bar\txEffect\tgets drunk\nPersonX decides to hire a tutor\txEffect\tlearns to read\nPersonX buys PersonY a gift\txEffect\tis thanked by PersonY\nPersonX hears bad news\txEffect\tfaints\nPersonX buys something\txEffect\tgets change\nPersonX does a lot of work\txEffect\tgets better at their job\nPersonX attends the concert\txEffect\tenjoys the music\nPersonX performs PersonX's duties\txEffect\treceives the expected salary\nPersonX gets calls\txEffect\thas a conversation\nPersonX continues PersonY's search\txEffect\tgets lost\nPersonX sees logs\txEffect\tcalms down\"\"\"\n    \nlines = [v.split('\\t') for v in  examples_string.split('\\n')]\n\nexamples = [{'head':v[0], 'relation':v[1], 'tail':v[2]} for v in lines]\n\nassert(examples[0]['relation'] == relation_to_test)\n\n\n\ndef ex2text(ex, include_gen = True):\n    \n    text = 'Because {},{}'.format(ex['head'], \n                                 ' she feels')\n    if include_gen:\n        text += ' {}\\n#\\n'.format(ex['tail'])\n    return text\n\n\ndef ex2text(ex, include_gen = True, number=None):\n\n    \n    text = ''\n    \n    if number is not None:\n        text += '{}.\\n\\n'.format(number)\n    \n    text += 'event: {}\\n\\noutcome: PersonX'.format(ex['head'],ex['head'])\n\n    if include_gen:\n        text += ' {}\\n\\n'.format(ex['tail'])\n    return text\n\n\n\nxEffect_dict = {'relation':relation_to_test, \n              'examples':examples[:10],\n             'function':ex2text,\n              'prefix':'What is the result?\\n\\n\\n'}\n\n\n\n\n\n\n\n\n'''\n\nADD xReact\n\n\n'''\n\n##### DEFINE THIS ######\nrelation_to_test = 'xReact'\n########################\n\n\n\nexamples_string = \"\"\"PersonX lives with PersonY's family\txReact\tloved\nPersonX expects to win\txReact\texcited\nPersonX comes home late\txReact\ttired\nPersonX sees dolphins\txReact\tjoyful\nPersonX causes PersonY anxiety\txReact\tguilty\nPersonX goes broke\txReact\tembarrassed\nPersonX has a drink\txReact\trefreshed\nPersonX has a heart condition\txReact\tscared about their health\nPersonX shaves PersonY's hair\txReact\thelpful\nPersonX loses all of PersonY's money\txReact\thorrible\nPersonX finishes dinner\txReact\trelaxed and content\nPersonX decides to go see a movie\txReact\timmersed in a different reality\nPersonX comes home early\txReact\thappy to be not at work\nPersonX finds peace\txReact\tin a position of grace\nPersonX says anything else\txReact\tglad that people listened to him\"\"\"\n    \nlines = [v.split('\\t') for v in  examples_string.split('\\n')]\n\nexamples = [{'head':v[0], 'relation':v[1], 'tail':v[2]} for v in lines]\n\nassert(examples[0]['relation'] == relation_to_test)\n\n##### DEFINE THIS ######\nrelation_to_test = 'xReact'\n########################\n\n\n\nexamples_string = \"\"\"PersonX lives with PersonY's family\txReact\tloved\nPersonX expects to win\txReact\texcited\nPersonX comes home late\txReact\ttired\nPersonX has a drink\txReact\trefreshed\nPersonX sees dolphins\txReact\tjoyful\nPersonX causes PersonY anxiety\txReact\tguilty\nPersonX goes broke\txReact\tembarrassed\nPersonX has a drink\txReact\trefreshed\nPersonX has a heart condition\txReact\tscared about their health\nPersonX shaves PersonY's hair\txReact\thelpful\nPersonX loses all PersonY's money\txReact\tbad they lost someone's belongings\nPersonX finishes dinner\txReact\trelaxed and content\nPersonX decides to go see a movie\txReact\timmersed in a different reality\nPersonX comes home early\txReact\thappy to be not at work\nPersonX finds peace\txReact\tin a position of grace\nPersonX says anything else\txReact\tglad that people listened to him\"\"\"\n    \nlines = [v.split('\\t') for v in  examples_string.split('\\n')]\n\nexamples = [{'head':v[0], 'relation':v[1], 'tail':v[2]} for v in lines]\n\nassert(examples[0]['relation'] == relation_to_test)\n\ndef ex2text(ex, include_gen = True):\n    \n    text = 'Because {},{}'.format(ex['head'], \n                                 ' she feels')\n    if include_gen:\n        text += ' {}\\n#\\n'.format(ex['tail'])\n    return text\n\n\ndef ex2text(ex, include_gen = True, number=None):\n\n    \n    text = ''\n    \n    if number is not None:\n        text += '{}.\\n\\n'.format(number)\n    \n    text += 'Event: {}\\n\\nOutcome: PersonX feels'.format(ex['head'])\n\n    if include_gen:\n        text += ' {}\\n\\n'.format(ex['tail'])\n    return text\n\n\n\nxReact_dict = {'relation':relation_to_test, \n              'examples':examples[:10],\n             'function':ex2text,\n              'prefix':'How do the characters react to the event?\\n\\n\\n'}\n\n\n\n\n\n\ndef ex2text(ex, include_gen = True):\n    \n    text = 'Because {},{}'.format(ex['head'], \n                                 ' she feels')\n    if include_gen:\n        text += ' {}\\n#\\n'.format(ex['tail'])\n    return text\n\n\ndef ex2text(ex, include_gen = True, number=None):\n\n    \n    text = ''\n    \n    if number is not None:\n        text += 'Situation {}: '.format(number)\n    \n    text += '{}.\\n\\nPersonX feels:'.format(ex['head'])\n\n    if include_gen:\n        text += ' {}.\\n\\n'.format(ex['tail'])\n    return text\n\n\n\nxReact_dict = {'relation':relation_to_test, \n              'examples':examples[:10],\n             'function':ex2text,\n              'prefix':'Next, how do people feel in each situation? Examples:\\n\\n\\n',\n              'length':8}\n\n\n\n\n\n\n'''\n\nADD xNeed\n\n'''\n\n'''\n\nADD xNeed\n\n'''\n\n##### DEFINE THIS ######\nrelation_to_test = 'xNeed'\n########################\n\n\n\nexamples_string = \"\"\"PersonX gets a job offer\txNeed\tto apply\nPersonX eats the food\txNeed\tto be hungry\nPersonX gets a date\txNeed\tto ask someone out\nPersonX has a baby shower\txNeed\tto invite people\nPersonX makes many new friends\txNeed\tto be social\nPersonX changes PersonY's mind\txNeed\tto make PersonY think about the issue\nPersonX watches Netflix\txNeed\tto have a Netflix account\nPersonX rides PersonY's skateboard\txNeed\tto be at the skate park\nPersonX takes a quick nap\txNeed\tto lie down\nPersonX handles the situation\txNeed\tto have an issue\nPersonX gets a new phone\txNeed\tto pay for the new phone\nPersonX takes the trash out\txNeed\tto gather the trash\"\"\"\n    \nlines = [v.split('\\t') for v in  examples_string.split('\\n')]\n\nexamples = [{'head':v[0], 'relation':v[1], 'tail':v[2]} for v in lines]\n\nassert(examples[0]['relation'] == relation_to_test)\n\n\ndef ex2text(ex, include_gen = True, number = None):\n\n    text = ''\n    if number is not None:\n        text += 'Event {}.: '.format(number)\n    text += '{}\\n\\nPrerequisites: For this to happen, PersonX needed'.format(ex['head'],ex['head'])\n\n    if include_gen:\n        text += ' {}.\\n\\n'.format(ex['tail'])\n    return text\n\n\n\nxNeed_dict = {'relation':relation_to_test, \n              'examples':examples[:10],\n             'function':ex2text,\n             'prefix':'What needs to be true for this event to take place?\\n\\n\\n'}\n\n\n\n\n\n\n\n\n'''\n\nADD HinderedBy\n\nTODO: select examples\n\n'''\n\n##### DEFINE THIS ######\nrelation_to_test = 'HinderedBy'\n########################\n\nexamples_string = \"\"\"PersonX makes a doctor's appointment\tHinderedBy\tPersonX can't find the phone to call the doctor\nPersonX rubs PersonY's forehead\tHinderedBy\tPersonX is afraid to touch PersonY\nPersonX eats peanut butter\tHinderedBy\tPersonX is allergic to peanuts\nPersonX looks perfect\tHinderedBy\tPersonX can\\'t find any makeup\nPersonX goes on a run\tHinderedBy\tPersonX has a knee injury\nPersonX takes PersonY to the emergency room\tHinderedBy\tPersonY has no health insurance to pay for medical care\nPersonX spends time with PersonY's family\tHinderedBy\tPersonY’s family doesn't like spending time with PersonX\nPersonX moves from place to place\tHinderedBy\tPersonX can\\'t afford to move\nPersonX protests the government\tHinderedBy\tPersonX is arrested\nPersonX has a huge fight\tHinderedBy\tPersonX does not like confrontation\nPersonX understands PersonY's feelings\tHinderedBy\tPersonY will not talk to PersonX\nPersonX asks PersonY a question\tHinderedBy\tPersonY cannot hear PersonX\nPersonX loves PersonY very much\tHinderedBy\tPersonY is mean to PersonX\nPersonX makes a request\tHinderedBy\tPersonX is afraid of being rejected\nPersonX meets PersonY's spouse\tHinderedBy\tPersonY's spouse is out of the country\nPersonX fills a waterbottle\tHinderedBy\tThere is no sink nearby to fill it\nPersonX schedules an appointment\tHinderedBy\tThe receptionist won't answer PersonX's calls\nPersonX becomes rich\tHinderedBy\tPersonX has no revenue\nPersonX gets the job done\tHinderedBy\tPersonX is too tired to work\nPersonX sits alone in a room\tHinderedBy\tPeople come into the room and bother PersonX\nPersonX evaluates PersonY’s performance\tHinderedBy\tPersonX misses PersonY's performance\nPersonX mows lawns\tHinderedBy\tPersonX's lawn mower breaks\"\"\"\n    \nlines = [v.split('\\t') for v in  examples_string.split('\\n')]\n\nexamples = [{'head':v[0], 'relation':v[1], 'tail':v[2]} for v in lines]\n\nassert(examples[0]['relation'] == relation_to_test)\n\n\n\n## converts an example into text for few-shot\ndef ex2text(ex, include_gen = True, number = None):\n    text = ''\n    if number is not None:\n        text += '{}. Event: '.format(number)\n        \n    #text += '\"{}\"\\n\\nCondition: \"{}\" will not occur under the condition that'.format(ex['head'],ex['head'],number)\n    text += '{}.\\n\\nCondition: \"{}\" is untrue because'.format(ex['head'],ex['head'],number)\n\n    if include_gen:\n        text += ' {}.\\n\\n\\n'.format(ex['tail'])\n    return text\n\n\n## dictionary defining few-shot template for HinderedBy relation\n## prefix will be put at the beginning, before few-shot examples\nHinderedBy_dict = {'relation':relation_to_test, \n              'examples':examples[:5],\n             'function':ex2text,\n                  'prefix': 'What condition stops this event from occurring?\\n\\n\\n'}\n\n\nrelation_dicts = (HinderedBy_dict,\n                 xNeed_dict, \n                 xWant_dict,\n                 xIntent_dict,\n                 xReact_dict,\n                 xAttr_dict,\n                 xEffect_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T04:50:23.924107Z","iopub.execute_input":"2026-02-03T04:50:23.924933Z","iopub.status.idle":"2026-02-03T04:50:23.951907Z","shell.execute_reply.started":"2026-02-03T04:50:23.924897Z","shell.execute_reply":"2026-02-03T04:50:23.951142Z"},"_kg_hide-input":false},"outputs":[],"execution_count":9},{"cell_type":"code","source":"'''\n\nFigure out which heads we need to generate for\n\n'''\n\nimport os\nimport json\n\n\n# variable definitions\nbatch_name = 'curie_inference_generation_0' ## define name of this batch\nengine = 'curie' # the GPT-3 engine to use\ntop_p = 0.5# top_p for nucleus sampling\nn_gen = 10 # number of generations for each event/relation pair\nPersonX = 'Alex' # name to use for PersonX (to make sentences more natural)\nPersonY = 'Chris' # name to use for PersonY\nlength = 12 # maximum token length of generations (number of words per generaation will be less)\n\nstop_token = '\\n\\n' # stop token during generation\n#d = HinderedBy_dict # which relation to useo\nn_examples_in = 5 # how many few-shot examples to use in the prompt\n\n# names to use in few-shot examples\nnames = ['Jean','Robin','Charlie', 'Ryan','Taylor','Jordan','Riley','Jamie','Leslie','Rowan',\n               'Adrian','Ali','Wyatt', 'Sydney','Stevie','Shiloh', 'Sam','Pat','Noel','Nicky','Max',\n               'Madison', 'Lindsay','Leslie','Lee','Jesse','Hunter','Glen','Devin','Avery']\n\n\n\nheads_file = '/kaggle/input/dataset-todo/todo_events (2).txt'\nout_file = 'inference_gens.jsonl'\nmeta_file = 'params.json'\n\n\n### save params if this is not done already (for records)\nif not os.path.isfile( meta_file):\n    relation_param_dict = [{key:d[key] for key in d.keys() if key !='function'} for d in  relation_dicts] \n    param_dict = {'batch_name':batch_name,\n                    'engine':engine,\n                    'top_p':top_p,\n                    'n_gen':n_gen,\n                    'PersonX':PersonX,\n                    'PersonY':PersonY,\n                    'n_examples_in':n_examples_in,\n                    'length':length,\n                  'stop_token':stop_token,\n                  'names':names,\n                 'relation_param_dict':relation_param_dict}\n    with open(meta_file, 'w') as f:\n        f.write(json.dumps(param_dict))\n        \n        \n### create output file if it does not exist\nif not os.path.isfile(out_file):\n    with open(out_file,'w') as f:\n        pass\n    \n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T04:51:21.845560Z","iopub.execute_input":"2026-02-03T04:51:21.846295Z","iopub.status.idle":"2026-02-03T04:51:21.853401Z","shell.execute_reply.started":"2026-02-03T04:51:21.846264Z","shell.execute_reply":"2026-02-03T04:51:21.852595Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\nimport random\nimport time\nimport json\nimport torch\nimport gc\n\nt_start = time.time()\nprint_step = 100\nRELATION_BATCH_SIZE = 3\n\nwith open(heads_file) as f: \n    heads_all = [head.strip() for head in f.readlines()]\n\nheads_done = []\ntry:\n    with open(out_file, 'r') as f:\n        for line in f:\n            try:\n                heads_done.append(json.loads(line)['head'])\n            except: pass\nexcept FileNotFoundError:\n    pass\n\nheads_todo = [head for head in heads_all if head not in heads_done]\nprint('{} heads todo'.format(len(heads_todo)))\n\ndef chunker(seq, size):\n    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n\nfor i in range(len(heads_todo)):\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    time.sleep(0.02)\n    \n    head_to_test = heads_todo[i]\n    ex_result = {'head': head_to_test, 'tails_by_relation': {}}\n\n    random.shuffle(names)\n    names_list = list(zip(names[:15], names[15:]))\n\n    relation_tasks = []\n    for d in relation_dicts:\n        examples_in = d['examples'][:n_examples_in]\n        prompt = few_shot_prompt(examples_in + [{'head':head_to_test}], d['function'], number=True,\n                                 Person_list=names_list[:len(examples_in)]+ [(PersonX,PersonY)])\n        prompt = d['prefix'] + prompt\n        prompt = name_PX_PY(prompt, PersonX, PersonY)\n        \n        relation_tasks.append({'d': d, 'prompt': prompt})\n\n    for batch_tasks in chunker(relation_tasks, RELATION_BATCH_SIZE):\n        batch_prompts = [t['prompt'] for t in batch_tasks]\n        \n        try:\n            result = complete_gpt3(batch_prompts, length, engine, top_p=top_p, num_log_probs=1, n=n_gen, stop=stop_token, echo=False)\n        except RuntimeError as e:\n            if \"out of memory\" in str(e):\n                torch.cuda.empty_cache()\n                result = complete_gpt3(batch_prompts, length, engine, top_p=top_p, num_log_probs=1, n=n_gen, stop=stop_token, echo=False)\n            else:\n                raise e\n\n        for idx, task in enumerate(batch_tasks):\n            d = task['d']\n            relation_outputs = []\n            \n            start_idx = idx * n_gen\n            choices = result['choices'][start_idx : start_idx + n_gen]\n            \n            for choice in choices:\n                out = choice['text']\n                try:\n                    end_ind = choice['logprobs']['text_offset'].index(max(choice['logprobs']['text_offset']))\n                    nll = sum(choice['logprobs']['token_logprobs'][:end_ind + 1])\n                except:\n                    nll = 0.0\n                \n                relation_outputs.append({'text':out, 'result':choice, 'nll':nll})\n\n            ex_result['tails_by_relation'][d['relation']] = {'relation':d['relation'], 'prompt':task['prompt'], 'tails':relation_outputs}\n        \n        del result\n        torch.cuda.empty_cache()\n\n    with open(out_file,'a') as f:\n        f.write(json.dumps(ex_result) + '\\n')\n        \n    if (i % print_step) == 0:\n        print('='*50)\n        print('time: {:.2f} min, avg_rate: {:.2f}'.format((time.time()-t_start)/60.,(time.time()-t_start)/60./(i+1)))\n        print('{}) {}'.format(i, ex_result['head']))\n        print('='*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T04:51:25.282656Z","iopub.execute_input":"2026-02-03T04:51:25.283446Z","iopub.status.idle":"2026-02-03T12:41:51.588634Z","shell.execute_reply.started":"2026-02-03T04:51:25.283412Z","shell.execute_reply":"2026-02-03T12:41:51.587949Z"}},"outputs":[{"name":"stdout","text":"953 heads todo\n==================================================\ntime: 0.52 min, avg_rate: 0.52\n0) PersonX is a good friend\n==================================================\n==================================================\ntime: 50.95 min, avg_rate: 0.50\n100) PersonX makes an offhand remark about PersonY's appearance\n==================================================\n==================================================\ntime: 100.73 min, avg_rate: 0.50\n200) PersonX has been fired from a job\n==================================================\n==================================================\ntime: 150.45 min, avg_rate: 0.50\n300) PersonX decides to go backpacking\n==================================================\n==================================================\ntime: 199.81 min, avg_rate: 0.50\n400) PersonX doesn't get the joke\n==================================================\n==================================================\ntime: 249.07 min, avg_rate: 0.50\n500) PersonX uses a lot of water\n==================================================\n==================================================\ntime: 298.14 min, avg_rate: 0.50\n600) PersonX wants to sleep all day\n==================================================\n==================================================\ntime: 347.04 min, avg_rate: 0.50\n700) PersonX plays the guitar\n==================================================\n==================================================\ntime: 396.06 min, avg_rate: 0.49\n800) PersonX goes to church\n==================================================\n==================================================\ntime: 445.04 min, avg_rate: 0.49\n900) PersonX gets a letter\n==================================================\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"'''\n\nONLY RUN once you have generated all inferences\n\nthis block takes the full set of generated inferences,\nand puts them in a standardized format, including removing inferences\nthat repeat for a given event/relation input.\n\nIt also automatically divides generations into a train/val/test split\nbased on the event (i.e. all inferences for a given event are sorted into the \nsame split)\n\ngenerations are saved as a jsonl file where each entry has the following keys:\n\nhead: the event for generation\nrelation: relation for generation\ntail: the generated inference\nsplit: the dataset split\n\n'''\n\nimport json\n    \n    \nfull_dataset_file = 'unique_dataset.jsonl'\n    \n\ndef name_PX_PY(s, PersonX, PersonY):\n    return s.replace('PersonX', PersonX).replace('PersonY', PersonY)\ndef scrub_PX_PY(s, PersonX, PersonY):\n    return s.replace(PersonX, 'PersonX').replace(PersonY, 'PersonY')\n    \ndef process_tail(tail):\n    if tail[-1] == '.':\n        tail = tail[:-1]\n    if tail[0] == ' ':\n        tail = tail[1:]\n    tail = scrub_PX_PY(tail, PersonX, PersonY)\n    \n    return tail\n\n\n\n\nimport json\nimport random\ni = 0\n\ndata_out = []\n\nwith open(out_file) as f, open(full_dataset_file,'w') as f_out:\n    for line in f:\n        \n        r = random.random()\n        if r< 0.8:\n            split = 'train'\n        elif r < 0.9:\n            split = 'val'\n        else:\n            split = 'test'\n\n\n        d = json.loads(line)\n\n        small_tails_by_relation = {}\n\n        for relation in d['tails_by_relation'].keys():\n            \n            # remove short inferences (degenerate)\n            d['tails_by_relation'][relation]['tails'] = [v for v in d['tails_by_relation'][relation]['tails'] if len(v['text']) > 2]\n            \n            small_tails_by_relation[relation] = {'relation':relation,\n                                                 'tails':[{'text':process_tail(v['text'])} for v in d['tails_by_relation'][relation]['tails']]}\n\n\n        \n        \n        for relation in small_tails_by_relation.keys():\n            inferences = [v['text'] for v in small_tails_by_relation[relation]['tails']]\n            \n            # do not include repeats\n            for inference in set(list(inferences)):\n                data_out.append({'split':split,'head':d['head'], 'relation':relation, 'inference':inference })\n        \n        new_d = {'split':split,'head':d['head'],'tails_by_relation':small_tails_by_relation}\n        \n    \n    for d in data_out:\n        f_out.write(json.dumps(d) + '\\n')\n\n        \nprint('total of {} unique generated examples written to {}'.format(len(data_out), full_dataset_file))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T12:59:47.050713Z","iopub.execute_input":"2026-02-03T12:59:47.051065Z","iopub.status.idle":"2026-02-03T12:59:47.673642Z","shell.execute_reply.started":"2026-02-03T12:59:47.051038Z","shell.execute_reply":"2026-02-03T12:59:47.672999Z"}},"outputs":[{"name":"stdout","text":"total of 39144 unique generated examples written to unique_dataset.jsonl\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import json  # This import was missing or lost\n\n# Save Raw Data\nwith open(\"raw_atomic10x.jsonl\", \"w\") as f:\n    for entry in raw_data:\n        f.write(json.dumps(entry) + \"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T12:59:53.125040Z","iopub.execute_input":"2026-02-03T12:59:53.125377Z","iopub.status.idle":"2026-02-03T12:59:53.132353Z","shell.execute_reply.started":"2026-02-03T12:59:53.125348Z","shell.execute_reply":"2026-02-03T12:59:53.131394Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_139/1900724870.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Save Raw Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"raw_atomic10x.jsonl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'raw_data' is not defined"],"ename":"NameError","evalue":"name 'raw_data' is not defined","output_type":"error"}],"execution_count":16}]}