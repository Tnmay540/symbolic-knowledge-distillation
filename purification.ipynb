{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14719696,"sourceType":"datasetVersion","datasetId":9405068},{"sourceId":740037,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":564521,"modelId":577012}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install names ftfy","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-03T13:15:48.360661Z","iopub.execute_input":"2026-02-03T13:15:48.361467Z","iopub.status.idle":"2026-02-03T13:15:55.587488Z","shell.execute_reply.started":"2026-02-03T13:15:48.361435Z","shell.execute_reply":"2026-02-03T13:15:55.586748Z"}},"outputs":[{"name":"stdout","text":"Collecting names\n  Downloading names-0.3.0.tar.gz (789 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m789.1/789.1 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting ftfy\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy) (0.2.14)\nDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: names\n  Building wheel for names (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for names: filename=names-0.3.0-py3-none-any.whl size=803681 sha256=7292fdfc1722a2815db0ca1d01d68fb67bc40c98bc084648146456d4bc28fe74\n  Stored in directory: /root/.cache/pip/wheels/c7/f0/8f/de9f15941cd988c39b82703fa04cb2d550ba5867f13c6da052\nSuccessfully built names\nInstalling collected packages: names, ftfy\nSuccessfully installed ftfy-6.3.1 names-0.3.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# --- FIX 1: FORCE LEGACY KERAS (Run this first!) ---\n!pip install -U tf-keras\nimport os\nos.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"  # This restores .load_model() support for folders\n\n# --- NOW IMPORT TENSORFLOW ---\nimport json\nimport numpy as np\nimport tensorflow as tf\nimport transformers\nimport tqdm\nimport math\nimport names  # ensure pip install names\nimport ftfy\nimport sklearn.metrics\n\n# ==========================================\n# CONFIGURATION\n# ==========================================\n\n# Path to your trained model FOLDER\n# Make sure this path is exactly correct from your 'Input' section\nMODEL_PATH = \"/kaggle/input/roberta22/tensorflow2/default/1/retrained_new_tf/model~model=roberta-large-mnli~lr=5e-06~bs=128~dropout=0.10\"\n\n# --- FIX 2: CORRECT MODEL TYPE FOR TOKENIZER ---\n# You cannot put the full file path here. It must be the HuggingFace model name.\nMODEL_TYPE = \"roberta-large-mnli\"\n\n# Path to the dataset you want to filter\nTO_FILTER_PATH = \"/kaggle/input/abcdddd/unique_dataset (1).jsonl\"\n\n# Path to training results (Set to None if you don't have it)\nMODEL_RESULTS_PATH = None \n\n# Batch size for prediction\nBATCH_SIZE = 128\nRECALLS = [.5, .6, .7, .8, .9]\n\n# ==========================================\n# HELPER CLASSES\n# ==========================================\n\n_RELATIONS = {\n    'HinderedBy': 'can be hindered by',\n    'xNeed': 'but before, PersonX needed',\n    'xWant': 'as a result, PersonX wants',\n    'xIntent': 'because PersonX wanted',\n    'xReact': 'as a result, PersonX feels',\n    'xAttr': 'so, PersonX is seen as',\n    'xEffect': 'as a result, PersonX'\n}\n\nclass TextIterator(tf.keras.utils.Sequence):\n    def __init__(self, texts, tokenizer, batch_size=32, shuffle=False):\n        self.texts = texts\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.tokenizer = tokenizer\n        self._to_string = self._to_string_main\n\n    def __len__(self):\n        return math.ceil(len(self.texts) / self.batch_size)\n\n    def _sep_pair_with_name(self, cur):\n        p1 = names.get_first_name()\n        p2 = p1\n        while p2 == p1:\n            p2 = names.get_first_name()\n        cur = cur.replace('PersonX', p1)\n        cur = cur.replace('PersonY', p2)\n        cur = ftfy.fix_text(cur)\n        cur = cur.split('**SEP**')\n        return cur\n\n    def _to_string_main(self, x):\n        rel_text = _RELATIONS.get(x['relation'], x['relation']) \n        # Check for 'inference' vs 'tail' key\n        tail_text = x.get('inference', x.get('tail', ''))\n        cur = '{}**SEP**{} {}'.format(x['head'], rel_text, tail_text)\n        return self._sep_pair_with_name(cur)\n\n    def __getitem__(self, idx):\n        batch = self.texts[idx * self.batch_size:(idx + 1) * self.batch_size]\n        texts = [self._to_string(b) for b in batch]\n        text_X = self.tokenizer(texts, return_tensors='np', padding=True)['input_ids']\n        return text_X\n\n# ==========================================\n# MAIN PREDICTION LOGIC\n# ==========================================\n\ndef main():\n    np.random.seed(1)\n    \n    # 1. Load Thresholds\n    mean_cutoffs, mean_precs = [], []\n    if MODEL_RESULTS_PATH and os.path.exists(MODEL_RESULTS_PATH):\n        print(f\"Loading thresholds from {MODEL_RESULTS_PATH}...\")\n        with open(MODEL_RESULTS_PATH) as f:\n            data = json.load(f)\n            val_preds = data['val_preds']\n            val_labels = data['val_labels']\n            val_ps, val_rs, val_thresh = sklearn.metrics.precision_recall_curve(y_true=val_labels, probas_pred=val_preds)\n            for r in RECALLS:\n                idx = 0\n                while idx < len(val_rs) and val_rs[idx] > r: idx += 1\n                if idx < len(val_thresh):\n                    mean_cutoffs.append(val_thresh[idx])\n                    mean_precs.append(val_ps[idx])\n                else:\n                    mean_cutoffs.append(0.5)\n                    mean_precs.append(0.0)\n    else:\n        print(\"Using default cutoff 0.5\")\n        mean_cutoffs = [0.5] * len(RECALLS)\n        mean_precs = [0.0] * len(RECALLS)\n\n    # 2. Load Data\n    print(f\"Loading data from {TO_FILTER_PATH}...\")\n    to_filter = []\n    with open(TO_FILTER_PATH) as f:\n        for line in tqdm.tqdm(f):\n            try:\n                c_jsonl = json.loads(line)\n                c_jsonl['valid'] = -1 \n                to_filter.append(c_jsonl)\n            except: pass\n\n    # 3. Load Model\n    print(f\"Loading tokenizer: {MODEL_TYPE}...\")\n    try:\n        tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_TYPE)\n    except Exception as e:\n        print(f\"Error loading tokenizer. Did you fix MODEL_TYPE? Error: {e}\")\n        return\n\n    pred_iter = TextIterator(to_filter, tokenizer, batch_size=BATCH_SIZE)\n    \n    print(f\"Loading model from {MODEL_PATH}...\")\n    # This load_model call will now work thanks to the legacy fix\n    keras_model = tf.keras.models.load_model(MODEL_PATH)\n\n    # 4. Predict\n    print(\"Running prediction (5 passes)...\")\n    preds = []\n    for idx in range(5):\n        print(f\"Pass {idx+1}/5...\")\n        p = keras_model.predict(pred_iter, verbose=1).flatten()\n        p = p[:len(to_filter)] \n        preds.append(p)\n    \n    preds = np.mean(np.array(preds), axis=0)\n\n    # 5. Save Results\n    for idx, p in enumerate(preds):\n        to_filter[idx]['p_valid_model'] = float(p)\n\n    base_name = TO_FILTER_PATH.split('/')[-1].split('.')[0]\n    \n    # Save probabilistic file\n    with open(f'/kaggle/working/{base_name}_with_prob_est.jsonl', 'w') as f:\n        for d in to_filter:\n            f.write(json.dumps(d) + '\\n')\n\n    # Save filtered file (Default > 0.5)\n    if MODEL_RESULTS_PATH is None:\n        valid_idxs = np.where(preds > 0.5)[0]\n        fname = f'/kaggle/working/{base_name}_filtered_threshold_0.5.jsonl'\n        print(f\"Saving {len(valid_idxs)} valid items to {fname}\")\n        with open(fname, 'w') as f:\n            for idx in valid_idxs:\n                f.write(json.dumps(to_filter[idx]) + '\\n')\n    else:\n        for recall, cutoff in zip(RECALLS, mean_cutoffs):\n            valid_idxs = np.where(preds > cutoff)[0]\n            fname = f'/kaggle/working/{base_name}_filtered_recall_{recall}.jsonl'\n            with open(fname, 'w') as f:\n                for idx in valid_idxs:\n                    f.write(json.dumps(to_filter[idx]) + '\\n')\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T13:16:02.049213Z","iopub.execute_input":"2026-02-03T13:16:02.050005Z","iopub.status.idle":"2026-02-03T13:28:49.310905Z","shell.execute_reply.started":"2026-02-03T13:16:02.049967Z","shell.execute_reply":"2026-02-03T13:28:49.310175Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tf-keras in /usr/local/lib/python3.12/dist-packages (2.19.0)\nCollecting tf-keras\n  Downloading tf_keras-2.20.1-py3-none-any.whl.metadata (1.8 kB)\nCollecting tensorflow<2.21,>=2.20 (from tf-keras)\n  Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.21,>=2.20->tf-keras) (25.9.23)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.6.0)\nRequirement already satisfied: google_pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.21,>=2.20->tf-keras) (18.1.1)\nRequirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.21,>=2.20->tf-keras) (26.0rc2)\nRequirement already satisfied: protobuf>=5.28.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.21,>=2.20->tf-keras) (5.29.5)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.32.5)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.21,>=2.20->tf-keras) (75.2.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.1.0)\nRequirement already satisfied: typing_extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.21,>=2.20->tf-keras) (4.15.0)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.0.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.75.1)\nCollecting tensorboard~=2.20.0 (from tensorflow<2.21,>=2.20->tf-keras)\n  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: keras>=3.10.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.10.0)\nRequirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.0.2)\nRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.15.1)\nRequirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.5.3)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow<2.21,>=2.20->tf-keras) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (14.2.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.0)\nRequirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.17.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2.6.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2026.1.4)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.9)\nRequirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (11.3.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.1.3)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.0.3)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (2.19.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.2)\nDownloading tf_keras-2.20.1-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.7/620.7 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tensorboard, tensorflow, tf-keras\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.19.0\n    Uninstalling tensorboard-2.19.0:\n      Successfully uninstalled tensorboard-2.19.0\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.19.0\n    Uninstalling tensorflow-2.19.0:\n      Successfully uninstalled tensorflow-2.19.0\n  Attempting uninstall: tf-keras\n    Found existing installation: tf_keras 2.19.0\n    Uninstalling tf_keras-2.19.0:\n      Successfully uninstalled tf_keras-2.19.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\ntensorflow-text 2.19.0 requires tensorflow<2.20,>=2.19.0, but you have tensorflow 2.20.0 which is incompatible.\ntensorflow-decision-forests 1.12.0 requires tensorflow==2.19.0, but you have tensorflow 2.20.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed tensorboard-2.20.0 tensorflow-2.20.0 tf-keras-2.20.1\nUsing default cutoff 0.5\nLoading data from /kaggle/input/abcdddd/unique_dataset (1).jsonl...\n","output_type":"stream"},{"name":"stderr","text":"39144it [00:00, 320522.41it/s]","output_type":"stream"},{"name":"stdout","text":"Loading tokenizer: roberta-large-mnli...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"027558d08bf7472da31b12d34da035b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/688 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5eb83f6d6d96488c93543a06caa24f0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be0c292c50da4f7fa5f5ab4d48d6a70d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11889b861fc04ae6ba289814b91a0e51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ad112a83f8b49ea9ae85f7e7a800f24"}},"metadata":{}},{"name":"stdout","text":"Loading model from /kaggle/input/roberta22/tensorflow2/default/1/retrained_new_tf/model~model=roberta-large-mnli~lr=5e-06~bs=128~dropout=0.10...\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1770124641.781480      55 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15511 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"Running prediction (5 passes)...\nPass 1/5...\n306/306 [==============================] - 134s 424ms/step\nPass 2/5...\n306/306 [==============================] - 130s 423ms/step\nPass 3/5...\n306/306 [==============================] - 130s 423ms/step\nPass 4/5...\n306/306 [==============================] - 130s 424ms/step\nPass 5/5...\n306/306 [==============================] - 130s 423ms/step\nSaving 32131 valid items to /kaggle/working/unique_dataset (1)_filtered_threshold_0.5.jsonl\n","output_type":"stream"}],"execution_count":2}]}