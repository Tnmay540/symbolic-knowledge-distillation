{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14691381,"sourceType":"datasetVersion","datasetId":9385353}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers==4.30.0 torch accelerate bitsandbytes\n!pip install ftfy names scipy scikit-learn\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install -U bitsandbytes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\n# --- 1. Load Teacher (GPT-J) Correctly in 8-bit ---\n# Use the original base model, not the pre-quantized hivemind version\nteacher_name = \"EleutherAI/gpt-j-6B\"\n\nprint(f\"Loading Teacher: {teacher_name} with on-the-fly 8-bit quantization...\")\n\ntokenizer = AutoTokenizer.from_pretrained(teacher_name)\n\n# Define the quantization configuration\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,              # Switch to 4-bit\n    bnb_4bit_quant_type=\"nf4\",      # Normalized Float 4 (better for accuracy)\n    bnb_4bit_compute_dtype=torch.float16, # Compute in FP16 for speed\n    bnb_4bit_use_double_quant=True, # Saves extra memory\n)\n\nteacher_model = AutoModelForCausalLM.from_pretrained(\n    teacher_name,\n    quantization_config=bnb_config, # Pass the config here\n    device_map=\"auto\",\n)\n\nprint(\"Model loaded successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T03:59:24.303018Z","iopub.execute_input":"2026-02-03T03:59:24.303589Z","iopub.status.idle":"2026-02-03T04:03:39.393985Z","shell.execute_reply.started":"2026-02-03T03:59:24.303553Z","shell.execute_reply":"2026-02-03T04:03:39.393297Z"}},"outputs":[{"name":"stdout","text":"Loading Teacher: EleutherAI/gpt-j-6B with on-the-fly 8-bit quantization...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/24.2G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7e96bc8dd4d49ecbbffa9b21b2bc127"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at EleutherAI/gpt-j-6B were not used when initializing GPTJForCausalLM: ['transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.12.attn.bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.13.attn.bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.14.attn.bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.15.attn.bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.16.attn.bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.17.attn.bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.18.attn.bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.19.attn.bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.20.attn.bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.21.attn.bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.22.attn.bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.23.attn.bias', 'transformer.h.23.attn.masked_bias', 'transformer.h.24.attn.bias', 'transformer.h.24.attn.masked_bias', 'transformer.h.25.attn.bias', 'transformer.h.25.attn.masked_bias', 'transformer.h.26.attn.bias', 'transformer.h.26.attn.masked_bias', 'transformer.h.27.attn.bias', 'transformer.h.27.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.masked_bias']\n- This IS expected if you are initializing GPTJForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing GPTJForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"Model loaded successfully!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"'''\n\nLoad exampe events for few-shot generation, and define\nfunction to convert these into few-shot prompts\n\n'''\n\n\nwith open('/kaggle/input/seed-events/seed_events.txt') as f:\n    seed_events = [line.strip() for line in f.readlines()]\n\n\n## converts an example into text for few-shot\ndef ex2text(ex, include_gen = True, number = None):\n    text = ''\n    if number is not None:\n        text += '{}. Event:'.format(number)\n        \n    text += ''.format()\n\n    if include_gen:\n        text += ' {}\\n\\n\\n'.format(ex)\n    return text\n\n\n## create a few-shot prompt with the final \n## example left open for generation\n##\n### note: ex2text should be a function that takes \n## an example from examples and produces a string \n## template. This should accept an argument, include_gen \n## which is a bool to decide whether to leave it open for\n## generation or include the gt\ndef few_shot_prompt(examples, ex2text, number = None, Person_list = None):\n    template_str = ''\n    \n    i = -1\n    for i, example in enumerate(examples[:-1]):\n        \n        if number:\n            ex_str = ex2text(example, include_gen = True, number = i + 1)\n        else:\n            ex_str = ex2text(example, include_gen = True)\n            \n            \n        if Person_list is not None:\n            ex_str = name_PX_PY(ex_str, Person_list[i][0],Person_list[i][1] )\n            \n        template_str += ex_str\n        \n    i = i + 1\n    \n    if number:\n        ex_str = ex2text(examples[-1], include_gen = False,number = i + 1)\n    else:\n        ex_str = ex2text(examples[-1], include_gen = False)\n        \n    if Person_list is not None:\n        ex_str = name_PX_PY(ex_str, Person_list[i][0],Person_list[i][1] )\n    \n    template_str += ex_str\n    \n    return template_str\n\n\ndef scrub_PX_PY(s, PersonX, PersonY):\n    return s.replace(PersonX, 'PersonX').replace(PersonY, 'PersonY')\n\ndef name_PX_PY(s, PersonX, PersonY):\n    return s.replace('PersonX', PersonX).replace('PersonY', PersonY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T04:03:43.603941Z","iopub.execute_input":"2026-02-03T04:03:43.604653Z","iopub.status.idle":"2026-02-03T04:03:43.622399Z","shell.execute_reply.started":"2026-02-03T04:03:43.604621Z","shell.execute_reply":"2026-02-03T04:03:43.621684Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"\nimport torch.nn.functional as F\n\n# --- Manual Implementation of compute_transition_scores ---\ndef compute_transition_scores(sequences, scores, normalize_logits=True):\n    \"\"\"\n    Fallback implementation for older transformers versions.\n    Calculates the log-probabilities of the selected tokens.\n    \"\"\"\n    # 1. Stack the scores tuple into a single tensor\n    # scores comes as a tuple of tensors (one per step)\n    # Shape becomes: (batch_size, generated_len, vocab_size)\n    scores_stack = torch.stack(scores, dim=1)\n    \n    # 2. Normalize logits to log-probs if requested (NLL requires this)\n    if normalize_logits:\n        scores_stack = F.log_softmax(scores_stack, dim=-1)\n        \n    # 3. Align sequences with scores\n    # sequences contains [prompt + generated]. We only need [generated].\n    gen_len = len(scores)\n    \n    # Take the last 'gen_len' tokens from the sequence\n    generated_ids = sequences[:, -gen_len:]\n    \n    # 4. Gather the scores for the tokens that were actually selected\n    # We use gather to pluck the specific score for each token ID\n    generated_ids = generated_ids.unsqueeze(-1) # Shape: (batch, gen_len, 1)\n    token_scores = torch.gather(scores_stack, 2, generated_ids) # Shape: (batch, gen_len, 1)\n    \n    return token_scores.squeeze(-1) # Shape: (batch, gen_len)\n\ncompute_transition_scores \n\nprint(\"Manual compute_transition_scores defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T04:03:47.835562Z","iopub.execute_input":"2026-02-03T04:03:47.835885Z","iopub.status.idle":"2026-02-03T04:03:47.841999Z","shell.execute_reply.started":"2026-02-03T04:03:47.835858Z","shell.execute_reply":"2026-02-03T04:03:47.841316Z"}},"outputs":[{"name":"stdout","text":"Manual compute_transition_scores defined.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"\n# 1. Define the replacement function with NLL calculation\ndef complete_gpt3(prompt, l, model_name, num_log_probs=None, n=1, top_p=0.9, **kwargs):\n    \"\"\"\n    Replaces OpenAI API call with local teacher_model generation.\n    Calculates REAL log-probabilities so NLL filtering works correctly.\n    \"\"\"\n    # Encode the few-shot prompt\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(teacher_model.device)\n    input_length = inputs.input_ids.shape[1]\n    \n    with torch.no_grad():\n        # Generate 'n' sequences with scores enabled\n        outputs = teacher_model.generate(\n            **inputs,\n            max_new_tokens=l,\n            do_sample=True,\n            top_p=top_p,\n            num_return_sequences=n,\n            pad_token_id=tokenizer.eos_token_id,\n            return_dict_in_generate=True, # REQUIRED for scores\n            output_scores=True            # REQUIRED for scores\n        )\n    \n    # Calculate the log-probabilities for the generated tokens\n    # transition_scores[i] = log P(token_i | context)\n    # These are negative numbers (e.g. -0.5). Summing them gives the NLL.\n    transition_scores = compute_transition_scores(\n        outputs.sequences, outputs.scores, normalize_logits=True\n    )\n\n    choices = []\n    \n    # Iterate over every generated sequence in the batch\n    for idx, sequence in enumerate(outputs.sequences):\n        # 1. Decode the generated text (excluding prompt)\n        generated_tokens = sequence[input_length:]\n        full_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n        \n        # Stop at newline (the notebook expects 1 event per line)\n        if '\\n' in full_text:\n            # If there's a newline, we technically only want the text before it.\n            # For simplicity in NLL calculation, we will just use the full scores \n            # returned for the tokens generated so far.\n            generated_text = full_text.split('\\n')[0]\n        else:\n            generated_text = full_text\n\n        # 2. Get the scores for these specific tokens\n        # transition_scores is shape (batch_size, generated_seq_len)\n        # We grab the row for this specific sequence\n        # Move to CPU and convert to list\n        current_scores = transition_scores[idx].cpu().numpy().tolist()\n        \n        # 3. Construct the Mock OpenAI Response\n        # We construct 'text_offset' as a simple range so the original code's \"max index\" logic works.\n        # The original code uses: end_ind = choice['logprobs']['text_offset'].index(max(...))\n        dummy_offsets = list(range(len(current_scores)))\n        \n        choice_obj = {\n            \"text\": generated_text, \n            \"finish_reason\": \"stop\",\n            \"logprobs\": {\n                # This dummy offset list ensures the NLL summation loop runs for the full length\n                \"text_offset\": dummy_offsets, \n                # CRITICAL: These are now the REAL log probabilities\n                \"token_logprobs\": current_scores \n            }\n        }\n        choices.append(choice_obj)\n\n    return {\"choices\": choices}\n\nprint(\"Advanced Monkey patch applied! 'complete_gpt3' now calculates real NLL.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T04:03:50.832669Z","iopub.execute_input":"2026-02-03T04:03:50.833446Z","iopub.status.idle":"2026-02-03T04:03:50.841413Z","shell.execute_reply.started":"2026-02-03T04:03:50.833403Z","shell.execute_reply":"2026-02-03T04:03:50.840752Z"}},"outputs":[{"name":"stdout","text":"Advanced Monkey patch applied! 'complete_gpt3' now calculates real NLL.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"engine = 'davinci' # gpt-3 model version to use\n\nlength= 12 # maximum token length of generations. 12 tokens is up to 12 words, but likely less\ntop_p = 0.9 # top_p to use for nucleus sampling\nn_gen = 100 # number of generations per-batch\npresence_penalty = 0.5 # discourage tokens in the prompt\nfrequency_penalty = 0.5 # discourage tokens in the prompt based on frequency\nPersonX = 'PersonX' # name to use for PersonX. Default is just \"PersonX\"\nPersonY = 'PersonY' # name to use for PersonY. Default is just \"PersonY\"\n\n\n\nn_examples_in = 10# how many few-shot examples to use per generation batch\nn_batches = 20 # how many generation batches to run\n\n\n\nbatches = []\n\nanon_heads = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T04:03:54.613921Z","iopub.execute_input":"2026-02-03T04:03:54.614677Z","iopub.status.idle":"2026-02-03T04:03:54.619212Z","shell.execute_reply.started":"2026-02-03T04:03:54.614642Z","shell.execute_reply":"2026-02-03T04:03:54.618377Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import json\nimport time\nimport random\nimport torch  # Required for GPU memory management\nimport gc     # Required for garbage collection\nimport os\n# Define output file path\noutput_filename = '/kaggle/working/generated_events.jsonl'\n\nif os.path.exists(output_filename):\n    os.remove(output_filename)\n# Use a set instead of a list for anon_heads to save memory and make lookups faster\nanon_heads = set() \n\nfor i in range(n_batches):\n    # sleep to prevent timeout from api\n    time.sleep(0.05)\n    \n    # randomize which events are used for generation\n    random.shuffle(seed_events)\n    examples_in = seed_events[:n_examples_in]\n\n    # use names defined above\n    names = (PersonX, PersonY)\n    \n    # define prompt based on examples and names\n    prompt = few_shot_prompt(examples_in + [''], ex2text, number=True)\n    prompt = name_PX_PY(prompt, names[0], names[1])\n\n    ## generate using the prompt\n    print('='*20 + f' Batch {i+1}/{n_batches} ' + '='*20)\n    # print(prompt) # Optional: Comment out to reduce clutter in logs\n    \n    result = complete_gpt3(prompt, length, engine, top_p=top_p, num_log_probs=1, n=n_gen, stop='\\n\\n', echo=False,\n                           frequency_penalty=frequency_penalty, presence_penalty=presence_penalty)\n\n    ### Process the output\n    outputs = []\n    for choice in result['choices']:\n        try:\n            out = choice['text']\n        except:\n            out = ''\n            \n        # Handle logprobs logic (assuming your monkey patch provides this structure)\n        try:\n            end_ind = choice['logprobs']['text_offset'].index(max(choice['logprobs']['text_offset']))\n            nll = sum(choice['logprobs']['token_logprobs'][:end_ind + 1])\n        except:\n            nll = 0.0 # Fallback if monkey patch doesn't provide perfect logprobs\n\n        text = choice['text']\n        anon_text = scrub_PX_PY(out, names[0], names[1]).strip()\n\n        ## Validations\n        if choice['finish_reason'] != 'stop' or not (anon_text.startswith('PersonX')):\n            continue\n\n        outputs.append({\n            'text': text,\n            'anon_text': anon_text,\n            'result': choice,\n            'nll': nll,\n            'prompt': prompt\n        })\n        \n        # Add to set for tracking uniqueness\n        anon_heads.add(anon_text)\n\n    # --- CRITICAL CHANGE 1: Write immediately to file ---\n    # We open in 'a' (append) mode so we add to the file batch by batch\n    batch_data = {'prompt': prompt, 'events': outputs}\n    with open(output_filename, 'a') as f:\n        f.write(json.dumps(batch_data) + '\\n')\n\n    print(f'Batch saved. Total unique events so far: {len(anon_heads)}')\n\n    # --- CRITICAL CHANGE 2: Clear Memory ---\n    # Delete large variables to free up RAM\n    del result\n    del outputs\n    del batch_data\n    \n    # Force Python's garbage collector to run\n    gc.collect()\n    \n    # Clear PyTorch's cache to free up GPU memory\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T04:03:57.373222Z","iopub.execute_input":"2026-02-03T04:03:57.373539Z","iopub.status.idle":"2026-02-03T04:10:49.119554Z","shell.execute_reply.started":"2026-02-03T04:03:57.373511Z","shell.execute_reply":"2026-02-03T04:10:49.118503Z"}},"outputs":[{"name":"stdout","text":"==================== Batch 1/20 ====================\nBatch saved. Total unique events so far: 80\n==================== Batch 2/20 ====================\nBatch saved. Total unique events so far: 169\n==================== Batch 3/20 ====================\nBatch saved. Total unique events so far: 253\n==================== Batch 4/20 ====================\nBatch saved. Total unique events so far: 346\n==================== Batch 5/20 ====================\nBatch saved. Total unique events so far: 434\n==================== Batch 6/20 ====================\nBatch saved. Total unique events so far: 518\n==================== Batch 7/20 ====================\nBatch saved. Total unique events so far: 588\n==================== Batch 8/20 ====================\nBatch saved. Total unique events so far: 677\n==================== Batch 9/20 ====================\nBatch saved. Total unique events so far: 754\n==================== Batch 10/20 ====================\nBatch saved. Total unique events so far: 830\n==================== Batch 11/20 ====================\nBatch saved. Total unique events so far: 911\n==================== Batch 12/20 ====================\nBatch saved. Total unique events so far: 976\n==================== Batch 13/20 ====================\nBatch saved. Total unique events so far: 1051\n==================== Batch 14/20 ====================\nBatch saved. Total unique events so far: 1128\n==================== Batch 15/20 ====================\nBatch saved. Total unique events so far: 1195\n==================== Batch 16/20 ====================\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_190/1478962568.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# print(prompt) # Optional: Comment out to reduce clutter in logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     result = complete_gpt3(prompt, length, engine, top_p=top_p, num_log_probs=1, n=n_gen, stop='\\n\\n', echo=False,\n\u001b[0m\u001b[1;32m     35\u001b[0m                            frequency_penalty=frequency_penalty, presence_penalty=presence_penalty)\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_190/3139471544.py\u001b[0m in \u001b[0;36mcomplete_gpt3\u001b[0;34m(prompt, l, model_name, num_log_probs, n, top_p, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Generate 'n' sequences with scores enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         outputs = teacher_model.generate(\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# 9. Call generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m         result = decoding_method(\n\u001b[0m\u001b[1;32m   2565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2783\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_prefill\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2784\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2785\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2786\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gptj/modeling_gptj.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0;31m# compute loss in fp32 to match with mesh-tf version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m         \u001b[0;31m# https://github.com/EleutherAI/gpt-neo/blob/89ce74164da2fb16179106f54e2269b5da8db333/models/gpt2/gpt2.py#L179\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.57 GiB. GPU 0 has a total capacity of 15.89 GiB of which 2.05 GiB is free. Process 4743 has 13.84 GiB memory in use. Of the allocated memory 10.75 GiB is allocated by PyTorch, and 2.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 2.57 GiB. GPU 0 has a total capacity of 15.89 GiB of which 2.05 GiB is free. Process 4743 has 13.84 GiB memory in use. Of the allocated memory 10.75 GiB is allocated by PyTorch, and 2.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":7},{"cell_type":"code","source":"import json\n\ngenerated_batches = []\n# Use 'generated_events.jsonl' directly as saved in the previous step\nwith open('/kaggle/working/generated_events.jsonl', 'r') as f:\n    for line in f: # Iterate line by line to save RAM\n        generated_batches.append(json.loads(line))\n\n# Extract events\nall_events_list = []\nfor batch in generated_batches:\n    for d in batch['events']:\n        all_events_list.append((d['anon_text'], d['nll']))\n\n# Deduplicate (Keep the last occurrence)\nall_events_dict = {v[0]: v[1] for v in all_events_list}\n# Convert back to list of tuples\nall_events = list(all_events_dict.items())\n\n# --- SAFETY CHECK FOR LOCAL MODEL ---\n# Check if NLLs are actually useful (not all 0.0)\nnll_values = [v[1] for v in all_events]\nif sum(nll_values) == 0:\n    print(\"Warning: All NLL values are 0.0 (likely from Monkey Patch). Skipping truncation.\")\n    # Do NOT truncate. Keep everything.\nelse:\n    # Sort Ascending (Lowest NLL is best)\n    all_events.sort(key=lambda v: v[1]) \n    \n    # Keep top 80% (The ones with lowest NLL)\n    cutoff = int(0.8 * len(all_events))\n    all_events = all_events[:cutoff]\n    print(f\"Truncated bottom 20%. Remaining events: {len(all_events)}\")\n\n# Prepare for next step\n# The next step likely expects a list of strings\nfinal_event_list = [v[0] for v in all_events]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T04:28:45.223952Z","iopub.execute_input":"2026-02-03T04:28:45.224568Z","iopub.status.idle":"2026-02-03T04:28:45.249352Z","shell.execute_reply.started":"2026-02-03T04:28:45.224537Z","shell.execute_reply":"2026-02-03T04:28:45.248765Z"}},"outputs":[{"name":"stdout","text":"Truncated bottom 20%. Remaining events: 956\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"''' \n\nFinally, remove any events with strange formatting or degenerate properties\n\n''' \nimport os\ntodo_events = []\n\noutput_filename = '/kaggle/working/todo_events.txt'\n\nif os.path.exists(output_filename):\n    os.remove(output_filename)\nfor event,_ in all_events:\n\n    if any(not (c.isalnum() or c in '\\',\".-â€™$ ') for c in event) or (len(event) < len(PersonX)):\n        continue\n    todo_events += [event]\n    \n\n# write these to a file for use in inference generation\nwith open('/kaggle/working/todo_events.txt','w') as f:\n    for event in todo_events:\n        f.write('{}\\n'.format(event))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T04:28:50.426432Z","iopub.execute_input":"2026-02-03T04:28:50.427141Z","iopub.status.idle":"2026-02-03T04:28:50.436364Z","shell.execute_reply.started":"2026-02-03T04:28:50.427099Z","shell.execute_reply":"2026-02-03T04:28:50.435643Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}